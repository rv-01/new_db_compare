# DB-Sentinel Configuration File
# ==============================
# 
# Enterprise database comparison tool supporting multiple tables
# with flexible configurations for each table.

# Source database connection
source_db:
  user: source_user
  password: source_password
  dsn: source_host:1521/SOURCEDB
  connection_timeout: 60
  query_timeout: 900

# Target database connection  
target_db:
  user: target_user
  password: target_password
  dsn: target_host:1521/TARGETDB
  connection_timeout: 60
  query_timeout: 900

# Global configuration settings
global_config:
  schema: HR                    # Default schema (can be overridden per table)
  max_threads: 6               # Default max threads (can be overridden per table)
  batch_timeout: 600           # Timeout for batch processing in seconds
  enable_restart: true         # Enable checkpoint-based restart capability
  enable_reverification: true  # Enable post-comparison verification
  enable_data_masking: false   # Enable PII masking (requires masking_config)
  output_directory: "./output" # Directory for generated SQL files
  log_directory: "./logs"      # Directory for log files
  archive_directory: "./archive" # Directory for archived data

# Table configurations - each table can have its own settings
tables:
  # Example 1: Simple table comparison with all columns
  - table_name: "EMPLOYEES"
    primary_key: ["EMPLOYEE_ID"]
    chunk_size: 10000
    # columns: null        # null means all columns will be compared
    # where_clause: null   # null means no filtering
    # schema: null         # null means use global schema
    # max_threads: null    # null means use global max_threads

  # Example 2: Table with composite primary key
  - table_name: "ORDER_ITEMS"
    primary_key: ["ORDER_ID", "ITEM_ID"]
    chunk_size: 5000
    # Optional: specify only certain columns to compare
    columns: ["ORDER_ID", "ITEM_ID", "QUANTITY", "PRICE", "STATUS"]

  # Example 3: Table with WHERE clause filter for recent data only
  - table_name: "TRANSACTIONS"
    primary_key: ["TRANSACTION_ID"]
    chunk_size: 15000
    where_clause: "TRANSACTION_DATE >= DATE '2024-01-01'"

  # Example 4: Large audit table with smaller chunks and specific columns
  - table_name: "AUDIT_LOG"
    primary_key: ["LOG_ID"]
    chunk_size: 2000
    columns: ["LOG_ID", "USER_ID", "ACTION", "TIMESTAMP", "STATUS"]
    where_clause: "STATUS = 'ACTIVE'"

  # Example 5: Table with different schema and custom thread count
  - table_name: "CUSTOMER_DATA"
    schema: "SALES"           # Override global schema
    primary_key: ["CUSTOMER_ID"]
    chunk_size: 8000
    max_threads: 8            # Override global max_threads
    columns: ["CUSTOMER_ID", "NAME", "EMAIL", "PHONE", "ADDRESS", "STATUS"]
    where_clause: "CREATED_DATE >= TRUNC(SYSDATE) - 30"  # Last 30 days

  # Example 6: Financial data with composite key and specific filtering
  - table_name: "ACCOUNT_BALANCES"
    schema: "FINANCE"
    primary_key: ["ACCOUNT_ID", "BALANCE_DATE"]
    chunk_size: 20000
    columns: ["ACCOUNT_ID", "BALANCE_DATE", "BALANCE_AMOUNT", "CURRENCY", "STATUS"]
    where_clause: "BALANCE_DATE >= TRUNC(SYSDATE, 'MM')"  # Current month

  # Example 7: Reference data table (small, frequent comparison)
  - table_name: "LOOKUP_CODES"
    primary_key: ["CODE_TYPE", "CODE_VALUE"]
    chunk_size: 1000
    # Compare all columns for reference data

  # Example 8: Large historical data with partitioning consideration
  - table_name: "SALES_HISTORY"
    schema: "ANALYTICS"
    primary_key: ["SALE_ID"]
    chunk_size: 25000
    max_threads: 10
    columns: ["SALE_ID", "SALE_DATE", "CUSTOMER_ID", "PRODUCT_ID", "QUANTITY", "AMOUNT"]
    where_clause: "SALE_DATE >= ADD_MONTHS(TRUNC(SYSDATE), -12)"  # Last 12 months

  # Example 9: Master data with specific business rules
  - table_name: "PRODUCT_MASTER"
    schema: "MASTER_DATA"
    primary_key: ["PRODUCT_ID"]
    chunk_size: 5000
    columns: ["PRODUCT_ID", "PRODUCT_NAME", "CATEGORY", "PRICE", "STATUS", "LAST_UPDATED"]
    where_clause: "STATUS IN ('ACTIVE', 'PENDING')"

  # Example 10: Complex table with multiple conditions
  - table_name: "USER_SESSIONS"
    schema: "APPLICATION"
    primary_key: ["SESSION_ID"]
    chunk_size: 12000
    max_threads: 4
    columns: ["SESSION_ID", "USER_ID", "LOGIN_TIME", "LOGOUT_TIME", "IP_ADDRESS", "STATUS"]
    where_clause: "LOGIN_TIME >= SYSDATE - INTERVAL '7' DAY AND STATUS != 'INVALID'"

# Optional: Data masking configuration (if enable_data_masking is true)
masking_config:
  enabled: false
  rules:
    - table_pattern: ".*CUSTOMER.*"
      column_rules:
        - column_name: "EMAIL"
          masking_strategy: "email_domain_preserve"
        - column_name: "PHONE"
          masking_strategy: "format_preserving_hash"
        - column_name: "ADDRESS"
          masking_strategy: "fake_address"

# Optional: Advanced performance settings
performance:
  connection_pool_size: 10
  query_timeout: 900
  batch_retry_count: 3
  parallel_degree: 4           # Oracle parallel query degree
  hash_algorithm: "md5"        # md5, sha1, sha256

# Optional: Monitoring and alerting
monitoring:
  enabled: true
  metrics_port: 8080
  health_check_interval: 30
  alert_on_failure: true
  alert_on_high_mismatch: true
  mismatch_threshold_percent: 5.0

# Optional: Output and reporting
reporting:
  generate_html_report: true
  generate_csv_summary: true
  include_sample_differences: true
  max_sample_differences: 100
  email_report: false
  email_recipients: []

# Optional: Integration settings
integrations:
  kafka:
    enabled: false
    bootstrap_servers: "localhost:9092"
    topic_prefix: "dbsentinel"
  
  prometheus:
    enabled: false
    push_gateway: "localhost:9091"
    job_name: "db-sentinel"
  
  slack:
    enabled: false
    webhook_url: ""
    channel: "#data-ops"

# Optional: Scheduling (for automated runs)
scheduling:
  enabled: false
  cron_expression: "0 2 * * *"  # Daily at 2 AM
  timezone: "UTC"
  max_runtime_hours: 4
